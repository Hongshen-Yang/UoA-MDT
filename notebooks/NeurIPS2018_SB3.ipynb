{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS2018_SB3\n",
    "\n",
    "This is a modification version based on the first notebook from [FinRL-tutorial](https://github.com/AI4Finance-Foundation/FinRL-Tutorials)\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/Stock_NeurIPS2018_SB3.ipynb\n",
    "\n",
    "## Part 1. Task Discription\n",
    "DRL agent training for cryptocurrency trading. \n",
    "\n",
    "This task is modeled as a Markov Decision Process (MDP), and the objective function is maximizing (expected) cumulative return.\n",
    "\n",
    "We specify the state-action-reward as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes many features and learns by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "\n",
    "selling, holding, and buying. When an action operates single crypto a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 units of BTC\" or \"Sell 10 units of BTC\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s', i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "**Market environment**: Cryptocurrencies from Binance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HSY/miniconda3/envs/uoa-mdt/lib/python3.10/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime, sqlite3, zipfile, os\n",
    "\n",
    "%matplotlib inline\n",
    "# from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "check_and_make_directories([TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "\n",
    "Read data from `binance-public-data`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is read from SQLite database\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('../sqlite.db')\n",
    "\n",
    "table_name = 'kline_copy'\n",
    "\n",
    "# Read data from the table into a DataFrame\n",
    "query = f'SELECT * FROM {table_name};'\n",
    "sql_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Create a new DataFrame with renamed columns\n",
    "df = sql_df[['start_time', 'open', 'high', 'low', 'close', 'base_vol', 'symbol', 'id']].rename(\n",
    "    columns={\n",
    "    'start_time': 'start_time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'base_vol': 'volume',\n",
    "    'symbol': 'tic',\n",
    "    'id': 'id'\n",
    "    })\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>7195.24</td>\n",
       "      <td>7255.0</td>\n",
       "      <td>7175.15</td>\n",
       "      <td>7200.85</td>\n",
       "      <td>16792.388165</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>7200.77</td>\n",
       "      <td>7212.5</td>\n",
       "      <td>6924.74</td>\n",
       "      <td>6965.71</td>\n",
       "      <td>31951.483932</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>6965.49</td>\n",
       "      <td>7405.0</td>\n",
       "      <td>6871.04</td>\n",
       "      <td>7344.96</td>\n",
       "      <td>68428.500451</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>7345.00</td>\n",
       "      <td>7404.0</td>\n",
       "      <td>7272.21</td>\n",
       "      <td>7354.11</td>\n",
       "      <td>29987.974977</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>7354.19</td>\n",
       "      <td>7495.0</td>\n",
       "      <td>7318.00</td>\n",
       "      <td>7358.75</td>\n",
       "      <td>38331.085604</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open    high      low    close        volume      tic  day\n",
       "0  2020-01-01  7195.24  7255.0  7175.15  7200.85  16792.388165  BTCUSDT    0\n",
       "1  2020-01-02  7200.77  7212.5  6924.74  6965.71  31951.483932  BTCUSDT    1\n",
       "2  2020-01-03  6965.49  7405.0  6871.04  7344.96  68428.500451  BTCUSDT    2\n",
       "3  2020-01-04  7345.00  7404.0  7272.21  7354.11  29987.974977  BTCUSDT    3\n",
       "4  2020-01-05  7354.19  7495.0  7318.00  7358.75  38331.085604  BTCUSDT    4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of symbols to merge\n",
    "symbols = ['BTCUSDT']\n",
    "\n",
    "# List to store individual DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each symbol\n",
    "for symbol in symbols:\n",
    "    directory = f'../mdt_utils/binance-public-data/python/data/spot/monthly/klines/{symbol}/1d/2020-01-01_2023-07-01/'\n",
    "    \n",
    "    # Loop through each zip file in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.zip'):\n",
    "            with zipfile.ZipFile(os.path.join(directory, file_name), 'r') as zip_ref:\n",
    "                # only one CSV file in each zip archive\n",
    "                csv_file = zip_ref.namelist()[0]\n",
    "                with zip_ref.open(csv_file) as csv_fp:\n",
    "                    # Read the CSV data into a DataFrame\n",
    "                    temp_df = pd.read_csv(csv_fp, header=None)\n",
    "                    temp_df.columns = ['open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "                    temp_df['date'] = pd.to_datetime(temp_df['close_time'], unit='ms').dt.strftime('%Y-%m-%d')\n",
    "                    temp_df['day'] = (pd.to_datetime(temp_df['date']) - pd.to_datetime(temp_df['date'].iloc[0])).dt.days\n",
    "                    temp_df['tic'] = symbol\n",
    "                    dfs.append(temp_df[['date', 'open', 'high', 'low', 'close', 'volume', 'tic', 'day']])\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df.sort_values(['date','tic'],ignore_index=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2020-01-01'\n",
    "TRAIN_END_DATE = '2023-12-31'\n",
    "TRADE_START_DATE = '2023-01-01'\n",
    "TRADE_END_DATE = '2023-07-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Preprocess Data\n",
    "\n",
    "TODO: The default feature engineering is based on date. I need to rewrite into timestamp based method\n",
    "\n",
    "The dafult [data split](https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/preprocessor/preprocessors.py) is not applicable here. Need to manually redo it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (899, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>7200.77</td>\n",
       "      <td>7212.50</td>\n",
       "      <td>6924.74</td>\n",
       "      <td>6965.71</td>\n",
       "      <td>31951.483932</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.275577</td>\n",
       "      <td>7415.818177</td>\n",
       "      <td>6750.741823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7083.280000</td>\n",
       "      <td>7083.280000</td>\n",
       "      <td>12.47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>6965.49</td>\n",
       "      <td>7405.00</td>\n",
       "      <td>6871.04</td>\n",
       "      <td>7344.96</td>\n",
       "      <td>68428.500451</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.038388</td>\n",
       "      <td>7553.380949</td>\n",
       "      <td>6787.632384</td>\n",
       "      <td>62.525554</td>\n",
       "      <td>48.566103</td>\n",
       "      <td>9.784200</td>\n",
       "      <td>7170.506667</td>\n",
       "      <td>7170.506667</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>7357.64</td>\n",
       "      <td>7795.34</td>\n",
       "      <td>7346.76</td>\n",
       "      <td>7758.00</td>\n",
       "      <td>54635.695316</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.994099</td>\n",
       "      <td>7847.466595</td>\n",
       "      <td>6813.326738</td>\n",
       "      <td>78.616437</td>\n",
       "      <td>144.230167</td>\n",
       "      <td>47.804389</td>\n",
       "      <td>7330.396667</td>\n",
       "      <td>7330.396667</td>\n",
       "      <td>13.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>7758.90</td>\n",
       "      <td>8207.68</td>\n",
       "      <td>7723.71</td>\n",
       "      <td>8145.28</td>\n",
       "      <td>91171.684661</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.818211</td>\n",
       "      <td>8222.855977</td>\n",
       "      <td>6670.761165</td>\n",
       "      <td>84.911918</td>\n",
       "      <td>170.739586</td>\n",
       "      <td>67.374223</td>\n",
       "      <td>7446.808571</td>\n",
       "      <td>7446.808571</td>\n",
       "      <td>13.79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>8145.92</td>\n",
       "      <td>8455.00</td>\n",
       "      <td>7870.00</td>\n",
       "      <td>8055.98</td>\n",
       "      <td>112622.642640</td>\n",
       "      <td>7.0</td>\n",
       "      <td>73.936775</td>\n",
       "      <td>8360.665473</td>\n",
       "      <td>6685.244527</td>\n",
       "      <td>79.340168</td>\n",
       "      <td>129.684605</td>\n",
       "      <td>73.697212</td>\n",
       "      <td>7522.955000</td>\n",
       "      <td>7522.955000</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>8054.72</td>\n",
       "      <td>8055.96</td>\n",
       "      <td>7750.00</td>\n",
       "      <td>7817.76</td>\n",
       "      <td>64239.519830</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70.769674</td>\n",
       "      <td>8363.588357</td>\n",
       "      <td>6747.833865</td>\n",
       "      <td>67.175888</td>\n",
       "      <td>66.925906</td>\n",
       "      <td>57.832118</td>\n",
       "      <td>7555.711111</td>\n",
       "      <td>7555.711111</td>\n",
       "      <td>12.54</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>7817.74</td>\n",
       "      <td>8199.00</td>\n",
       "      <td>7672.00</td>\n",
       "      <td>8197.02</td>\n",
       "      <td>82406.777448</td>\n",
       "      <td>9.0</td>\n",
       "      <td>87.028833</td>\n",
       "      <td>8482.777731</td>\n",
       "      <td>6756.906269</td>\n",
       "      <td>73.793294</td>\n",
       "      <td>83.067127</td>\n",
       "      <td>62.228124</td>\n",
       "      <td>7619.842000</td>\n",
       "      <td>7619.842000</td>\n",
       "      <td>12.56</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>8184.97</td>\n",
       "      <td>8196.00</td>\n",
       "      <td>8055.89</td>\n",
       "      <td>8110.34</td>\n",
       "      <td>31159.755683</td>\n",
       "      <td>12.0</td>\n",
       "      <td>97.723665</td>\n",
       "      <td>8594.488743</td>\n",
       "      <td>6869.165103</td>\n",
       "      <td>67.344654</td>\n",
       "      <td>73.558586</td>\n",
       "      <td>59.274326</td>\n",
       "      <td>7731.826923</td>\n",
       "      <td>7731.826923</td>\n",
       "      <td>12.32</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>8110.34</td>\n",
       "      <td>8880.00</td>\n",
       "      <td>8105.54</td>\n",
       "      <td>8810.01</td>\n",
       "      <td>120399.126742</td>\n",
       "      <td>13.0</td>\n",
       "      <td>137.699470</td>\n",
       "      <td>8818.333521</td>\n",
       "      <td>6799.346479</td>\n",
       "      <td>75.718364</td>\n",
       "      <td>137.207864</td>\n",
       "      <td>74.380090</td>\n",
       "      <td>7808.840000</td>\n",
       "      <td>7808.840000</td>\n",
       "      <td>12.39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>8814.64</td>\n",
       "      <td>8916.48</td>\n",
       "      <td>8564.00</td>\n",
       "      <td>8821.41</td>\n",
       "      <td>84816.297606</td>\n",
       "      <td>14.0</td>\n",
       "      <td>166.766812</td>\n",
       "      <td>8980.744388</td>\n",
       "      <td>6771.944946</td>\n",
       "      <td>75.822861</td>\n",
       "      <td>144.724492</td>\n",
       "      <td>74.910640</td>\n",
       "      <td>7876.344667</td>\n",
       "      <td>7876.344667</td>\n",
       "      <td>12.42</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      tic     open     high      low    close         volume  \\\n",
       "0  2020-01-02  BTCUSDT  7200.77  7212.50  6924.74  6965.71   31951.483932   \n",
       "1  2020-01-03  BTCUSDT  6965.49  7405.00  6871.04  7344.96   68428.500451   \n",
       "2  2020-01-06  BTCUSDT  7357.64  7795.34  7346.76  7758.00   54635.695316   \n",
       "3  2020-01-07  BTCUSDT  7758.90  8207.68  7723.71  8145.28   91171.684661   \n",
       "4  2020-01-08  BTCUSDT  8145.92  8455.00  7870.00  8055.98  112622.642640   \n",
       "5  2020-01-09  BTCUSDT  8054.72  8055.96  7750.00  7817.76   64239.519830   \n",
       "6  2020-01-10  BTCUSDT  7817.74  8199.00  7672.00  8197.02   82406.777448   \n",
       "7  2020-01-13  BTCUSDT  8184.97  8196.00  8055.89  8110.34   31159.755683   \n",
       "8  2020-01-14  BTCUSDT  8110.34  8880.00  8105.54  8810.01  120399.126742   \n",
       "9  2020-01-15  BTCUSDT  8814.64  8916.48  8564.00  8821.41   84816.297606   \n",
       "\n",
       "    day        macd      boll_ub      boll_lb     rsi_30      cci_30  \\\n",
       "0   1.0   -5.275577  7415.818177  6750.741823   0.000000  -66.666667   \n",
       "1   2.0    5.038388  7553.380949  6787.632384  62.525554   48.566103   \n",
       "2   5.0   30.994099  7847.466595  6813.326738  78.616437  144.230167   \n",
       "3   6.0   59.818211  8222.855977  6670.761165  84.911918  170.739586   \n",
       "4   7.0   73.936775  8360.665473  6685.244527  79.340168  129.684605   \n",
       "5   8.0   70.769674  8363.588357  6747.833865  67.175888   66.925906   \n",
       "6   9.0   87.028833  8482.777731  6756.906269  73.793294   83.067127   \n",
       "7  12.0   97.723665  8594.488743  6869.165103  67.344654   73.558586   \n",
       "8  13.0  137.699470  8818.333521  6799.346479  75.718364  137.207864   \n",
       "9  14.0  166.766812  8980.744388  6771.944946  75.822861  144.724492   \n",
       "\n",
       "        dx_30  close_30_sma  close_60_sma    vix  turbulence  \n",
       "0  100.000000   7083.280000   7083.280000  12.47         0.0  \n",
       "1    9.784200   7170.506667   7170.506667  14.02         0.0  \n",
       "2   47.804389   7330.396667   7330.396667  13.85         0.0  \n",
       "3   67.374223   7446.808571   7446.808571  13.79         0.0  \n",
       "4   73.697212   7522.955000   7522.955000  13.45         0.0  \n",
       "5   57.832118   7555.711111   7555.711111  12.54         0.0  \n",
       "6   62.228124   7619.842000   7619.842000  12.56         0.0  \n",
       "7   59.274326   7731.826923   7731.826923  12.32         0.0  \n",
       "8   74.380090   7808.840000   7808.840000  12.39         0.0  \n",
       "9   74.910640   7876.344667   7876.344667  12.42         0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full.sort_values(['date','tic'],ignore_index=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Build A Market Environment in OpenAI Gym-style\n",
    "The training process involves observing cryptocurrency price change, taking an action and reward's calculation. By interacting with the market environment, the agent will eventually derive a trading strategy that may maximize (expected) rewards.\n",
    "\n",
    "Our market environment, based on OpenAI Gym, simulates stock markets with historical market data.\n",
    "\n",
    "### Data Split\n",
    "We split the data into training set and testing set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data length: 899\n",
      "Trade Data Length: 143\n",
      "Indicators: ['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma']\n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE, TRADE_END_DATE)\n",
    "train_length = len(train)\n",
    "trade_length = len(trade)\n",
    "print(f\"Training Data length: {train_length}\")\n",
    "print(f\"Trade Data Length: {trade_length}\")\n",
    "print(f\"Indicators: {INDICATORS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crypto Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Crypto Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Train DRL Agents\n",
    "* The DRL algorithms are from Stable Baselines 3. Users are also encouraged to try ElegantRL and Ray RLlib.\n",
    "* FinRL includes fine-tuned standard DRL algorithms, such as DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "design their own DRL algorithms by adapting these DRL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 1: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1597     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.43    |\n",
      "|    explained_variance | 0.117    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.07     |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 30.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1751      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.983    |\n",
      "|    reward             | 1.1349653 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.87      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1815       |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.42      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -19.5      |\n",
      "|    reward             | -13.194996 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.36e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1854     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | -0.0897  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 8.27     |\n",
      "|    reward             | 1.82298  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 46.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1875     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0.00716  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -3.19    |\n",
      "|    reward             | -1.40722 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 12.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1890      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 72.9      |\n",
      "|    reward             | 26.561304 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.07e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1894      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -12.5     |\n",
      "|    reward             | 53.101387 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 154       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1903      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.41     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -16.4     |\n",
      "|    reward             | -5.146757 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 241       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1906       |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.41      |\n",
      "|    explained_variance | -0.434     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -4         |\n",
      "|    reward             | -0.3665375 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1864     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | -0.0522  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 63       |\n",
      "|    reward             | 4.422981 |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 2.04e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1875     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | -0.0171  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 6.28     |\n",
      "|    reward             | 1.670352 |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 44.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1884     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -2.38    |\n",
      "|    reward             | -7.43015 |\n",
      "|    std                | 0.985    |\n",
      "|    value_loss         | 118      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1893      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 7.73      |\n",
      "|    reward             | 8.234118  |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 37.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1900     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -2.96    |\n",
      "|    reward             | 1.59185  |\n",
      "|    std                | 0.985    |\n",
      "|    value_loss         | 7.66     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1905       |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.41      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 50.3       |\n",
      "|    reward             | -28.317636 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.05e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1909     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 44.6     |\n",
      "|    reward             | 4.602884 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 1.65e+03 |\n",
      "------------------------------------\n",
      "day: 898, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4181005.70\n",
      "total_reward: 3181005.70\n",
      "total_cost: 5997.83\n",
      "total_trades: 898\n",
      "Sharpe: 0.935\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1908       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.41      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 68.1       |\n",
      "|    reward             | -16.633692 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 4.4e+03    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1914     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | -0.22    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 2.7      |\n",
      "|    reward             | -1.38918 |\n",
      "|    std                | 0.986    |\n",
      "|    value_loss         | 24.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1919     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 6.56e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -30.2    |\n",
      "|    reward             | 3.507337 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 750      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1924     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 2.06     |\n",
      "|    reward             | -8.52486 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 16.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1929     |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -24.1    |\n",
      "|    reward             | 23.58775 |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 110      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1932     |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 16.7     |\n",
      "|    reward             | 6.685656 |\n",
      "|    std                | 0.974    |\n",
      "|    value_loss         | 186      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1935     |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -2.32    |\n",
      "|    reward             | 4.272912 |\n",
      "|    std                | 0.969    |\n",
      "|    value_loss         | 8.95     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1937      |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -92       |\n",
      "|    reward             | -0.083076 |\n",
      "|    std                | 0.955     |\n",
      "|    value_loss         | 4.19e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1940      |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 42.5      |\n",
      "|    reward             | -4.547928 |\n",
      "|    std                | 0.948     |\n",
      "|    value_loss         | 1.51e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1943       |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.36      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | -20.9      |\n",
      "|    reward             | -25.589245 |\n",
      "|    std                | 0.946      |\n",
      "|    value_loss         | 502        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1945     |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 8.76     |\n",
      "|    reward             | 0.486168 |\n",
      "|    std                | 0.946    |\n",
      "|    value_loss         | 54.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1947      |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -51.4     |\n",
      "|    reward             | -9.980544 |\n",
      "|    std                | 0.928     |\n",
      "|    value_loss         | 1.65e+03  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1949      |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -6.36     |\n",
      "|    reward             | -1.111362 |\n",
      "|    std                | 0.941     |\n",
      "|    value_loss         | 51        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1951       |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.37      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 26.4       |\n",
      "|    reward             | -93.123024 |\n",
      "|    std                | 0.957      |\n",
      "|    value_loss         | 527        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1953      |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 8.6       |\n",
      "|    reward             | -0.43992  |\n",
      "|    std                | 0.962     |\n",
      "|    value_loss         | 52.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1955     |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 18.4     |\n",
      "|    reward             | 13.26387 |\n",
      "|    std                | 0.965    |\n",
      "|    value_loss         | 149      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1956      |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 70.3      |\n",
      "|    reward             | 29.949387 |\n",
      "|    std                | 0.971     |\n",
      "|    value_loss         | 4.31e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1958      |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -11.4     |\n",
      "|    reward             | -9.381576 |\n",
      "|    std                | 0.966     |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "day: 898, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1959      |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 6.52      |\n",
      "|    reward             | 1.616326  |\n",
      "|    std                | 0.969     |\n",
      "|    value_loss         | 911       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1961      |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -4.32     |\n",
      "|    reward             | -2.260512 |\n",
      "|    std                | 0.97      |\n",
      "|    value_loss         | 20.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1962     |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -9.72    |\n",
      "|    reward             | 4.201377 |\n",
      "|    std                | 0.968    |\n",
      "|    value_loss         | 136      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1963     |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -1.29    |\n",
      "|    reward             | -0.6533  |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 2        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1965      |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -164      |\n",
      "|    reward             | 3.540469  |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 1.18e+04  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1965     |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 10.5     |\n",
      "|    reward             | 13.24134 |\n",
      "|    std                | 0.989    |\n",
      "|    value_loss         | 116      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1965     |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 14.8     |\n",
      "|    reward             | -7.43176 |\n",
      "|    std                | 0.983    |\n",
      "|    value_loss         | 109      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1966     |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -1.99    |\n",
      "|    reward             | 2.826345 |\n",
      "|    std                | 0.97     |\n",
      "|    value_loss         | 265      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1966     |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -1.23    |\n",
      "|    reward             | 7.939428 |\n",
      "|    std                | 0.964    |\n",
      "|    value_loss         | 60.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1966       |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.39      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -12.4      |\n",
      "|    reward             | -11.497422 |\n",
      "|    std                | 0.971      |\n",
      "|    value_loss         | 316        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1967     |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 3.27     |\n",
      "|    reward             | 0.588393 |\n",
      "|    std                | 0.969    |\n",
      "|    value_loss         | 38.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1967     |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.39    |\n",
      "|    explained_variance | 3.63e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -57.5    |\n",
      "|    reward             | 3.210429 |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 2.6e+03  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1967      |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -4.14     |\n",
      "|    reward             | -1.934618 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1968      |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -0.000143 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -17.5     |\n",
      "|    reward             | -6.134891 |\n",
      "|    std                | 0.972     |\n",
      "|    value_loss         | 318       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1968      |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 34        |\n",
      "|    reward             | 11.167341 |\n",
      "|    std                | 0.968     |\n",
      "|    value_loss         | 520       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1967     |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -26.3    |\n",
      "|    reward             | 0.111249 |\n",
      "|    std                | 0.967    |\n",
      "|    value_loss         | 1.32e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1967       |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.38      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 41.2       |\n",
      "|    reward             | -29.563047 |\n",
      "|    std                | 0.965      |\n",
      "|    value_loss         | 2.38e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1968      |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 41.2      |\n",
      "|    reward             | 13.391334 |\n",
      "|    std                | 0.961     |\n",
      "|    value_loss         | 708       |\n",
      "-------------------------------------\n",
      "day: 898, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1968      |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -40       |\n",
      "|    reward             | 14.239449 |\n",
      "|    std                | 0.955     |\n",
      "|    value_loss         | 1.39e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1969     |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 6.29     |\n",
      "|    reward             | 1.617693 |\n",
      "|    std                | 0.954    |\n",
      "|    value_loss         | 30.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1969     |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 18.1     |\n",
      "|    reward             | 2.8623   |\n",
      "|    std                | 0.959    |\n",
      "|    value_loss         | 366      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1970     |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -2.03    |\n",
      "|    reward             | 0.788331 |\n",
      "|    std                | 0.961    |\n",
      "|    value_loss         | 4.04     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1970     |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 5.19e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -10.3    |\n",
      "|    reward             | 5.480388 |\n",
      "|    std                | 0.943    |\n",
      "|    value_loss         | 86.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1971     |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 28.2     |\n",
      "|    reward             | 6.191733 |\n",
      "|    std                | 0.949    |\n",
      "|    value_loss         | 250      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1971      |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -1.62     |\n",
      "|    reward             | -4.360143 |\n",
      "|    std                | 0.939     |\n",
      "|    value_loss         | 9.2       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1972       |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.36      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -84.2      |\n",
      "|    reward             | -18.206202 |\n",
      "|    std                | 0.948      |\n",
      "|    value_loss         | 1.04e+04   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1972      |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 3.54      |\n",
      "|    reward             | 11.137026 |\n",
      "|    std                | 0.94      |\n",
      "|    value_loss         | 43.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1973     |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 37.1     |\n",
      "|    reward             | 39.61014 |\n",
      "|    std                | 0.937    |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1973      |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 3.51      |\n",
      "|    reward             | -0.294972 |\n",
      "|    std                | 0.937     |\n",
      "|    value_loss         | 17.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1974      |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 71.3      |\n",
      "|    reward             | 28.325771 |\n",
      "|    std                | 0.934     |\n",
      "|    value_loss         | 5.01e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1974     |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | -0.598   |\n",
      "|    reward             | 0.29046  |\n",
      "|    std                | 0.928    |\n",
      "|    value_loss         | 2.5      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1974      |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.33     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 9.01      |\n",
      "|    reward             | 12.776292 |\n",
      "|    std                | 0.913     |\n",
      "|    value_loss         | 89.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1975     |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 4.37     |\n",
      "|    reward             | 3.062097 |\n",
      "|    std                | 0.914    |\n",
      "|    value_loss         | 189      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1975      |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -2.07     |\n",
      "|    reward             | -1.782099 |\n",
      "|    std                | 0.911     |\n",
      "|    value_loss         | 28.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1975       |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.33      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 23.5       |\n",
      "|    reward             | -14.698263 |\n",
      "|    std                | 0.918      |\n",
      "|    value_loss         | 524        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1976     |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | -2       |\n",
      "|    reward             | 8.48256  |\n",
      "|    std                | 0.92     |\n",
      "|    value_loss         | 261      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 898, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1975     |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -25      |\n",
      "|    reward             | 7.018134 |\n",
      "|    std                | 0.918    |\n",
      "|    value_loss         | 2.45e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1975     |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -3.47    |\n",
      "|    reward             | 3.134853 |\n",
      "|    std                | 0.916    |\n",
      "|    value_loss         | 34.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1973       |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.33      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 22         |\n",
      "|    reward             | -13.822512 |\n",
      "|    std                | 0.919      |\n",
      "|    value_loss         | 383        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1971      |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -1.64     |\n",
      "|    reward             | -0.923127 |\n",
      "|    std                | 0.931     |\n",
      "|    value_loss         | 3.02      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1970      |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -13.9     |\n",
      "|    reward             | -2.443812 |\n",
      "|    std                | 0.92      |\n",
      "|    value_loss         | 200       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 26.918169 |\n",
      "|    std                | 0.923     |\n",
      "|    value_loss         | 73.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1968     |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 8.86e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 3.6      |\n",
      "|    reward             | 1.145907 |\n",
      "|    std                | 0.934    |\n",
      "|    value_loss         | 31.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1968     |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 7.62     |\n",
      "|    reward             | 0.559065 |\n",
      "|    std                | 0.923    |\n",
      "|    value_loss         | 854      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1968      |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -3.96     |\n",
      "|    reward             | -0.571332 |\n",
      "|    std                | 0.919     |\n",
      "|    value_loss         | 65.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1968     |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -20.3    |\n",
      "|    reward             | 54.18644 |\n",
      "|    std                | 0.922    |\n",
      "|    value_loss         | 713      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1969       |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.33      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | -1.71      |\n",
      "|    reward             | -16.948341 |\n",
      "|    std                | 0.919      |\n",
      "|    value_loss         | 35.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -61.2     |\n",
      "|    reward             | -7.447902 |\n",
      "|    std                | 0.924     |\n",
      "|    value_loss         | 4.57e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1969     |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 1.74     |\n",
      "|    reward             | 3.325485 |\n",
      "|    std                | 0.919    |\n",
      "|    value_loss         | 9.07     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | 1.35e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 41.8      |\n",
      "|    reward             | 23.938698 |\n",
      "|    std                | 0.909     |\n",
      "|    value_loss         | 832       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.33     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 54.1      |\n",
      "|    reward             | -8.150223 |\n",
      "|    std                | 0.911     |\n",
      "|    value_loss         | 1.34e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 3.31      |\n",
      "|    reward             | -2.724684 |\n",
      "|    std                | 0.91      |\n",
      "|    value_loss         | 6.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -72.3     |\n",
      "|    reward             | 54.095356 |\n",
      "|    std                | 0.91      |\n",
      "|    value_loss         | 7.45e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1969     |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -17.4    |\n",
      "|    reward             | -1.80903 |\n",
      "|    std                | 0.907    |\n",
      "|    value_loss         | 329      |\n",
      "------------------------------------\n",
      "day: 898, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 157       |\n",
      "|    reward             | 19.317987 |\n",
      "|    std                | 0.91      |\n",
      "|    value_loss         | 1.62e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | -16.3     |\n",
      "|    reward             | -7.749783 |\n",
      "|    std                | 0.929     |\n",
      "|    value_loss         | 207       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 78.5      |\n",
      "|    reward             | 45.27623  |\n",
      "|    std                | 0.932     |\n",
      "|    value_loss         | 3.74e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1968      |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 8.96      |\n",
      "|    reward             | -2.372043 |\n",
      "|    std                | 0.935     |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1969      |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 12.1      |\n",
      "|    reward             | -2.388822 |\n",
      "|    std                | 0.935     |\n",
      "|    value_loss         | 225       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1966     |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 3.38     |\n",
      "|    reward             | 0.678069 |\n",
      "|    std                | 0.935    |\n",
      "|    value_loss         | 163      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1966      |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | -22.6     |\n",
      "|    reward             | -1.017174 |\n",
      "|    std                | 0.949     |\n",
      "|    value_loss         | 213       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1965       |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.35      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -103       |\n",
      "|    reward             | -10.038213 |\n",
      "|    std                | 0.936      |\n",
      "|    value_loss         | 7.96e+03   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1964     |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -4.27    |\n",
      "|    reward             | 5.221794 |\n",
      "|    std                | 0.941    |\n",
      "|    value_loss         | 35.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1963      |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 29.3      |\n",
      "|    reward             | 24.263561 |\n",
      "|    std                | 0.928     |\n",
      "|    value_loss         | 1.15e+03  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1964     |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | 1.47e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -23.6    |\n",
      "|    reward             | 3.651195 |\n",
      "|    std                | 0.931    |\n",
      "|    value_loss         | 1.03e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1964      |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 41.3      |\n",
      "|    reward             | 25.854748 |\n",
      "|    std                | 0.933     |\n",
      "|    value_loss         | 1.49e+03  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 898, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 341      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 3596     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.6e+03  |\n",
      "|    critic_loss     | 7.52e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2697     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 301      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 7192     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.31e+03 |\n",
      "|    critic_loss     | 2.52e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6293     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 291      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 10788    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.1e+03  |\n",
      "|    critic_loss     | 5.84e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9889     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 288      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 14384    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 916      |\n",
      "|    critic_loss     | 3.21e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13485    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 285      |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 17980    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 19       |\n",
      "|    critic_loss     | 122      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17081    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 284      |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 21576    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 11.6     |\n",
      "|    critic_loss     | 63.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20677    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 283      |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 25172    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.67     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24273    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 283      |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 28768    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.3      |\n",
      "|    critic_loss     | 15.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27869    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 282      |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 32364    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.84     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31465    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 281      |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 35960    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.8      |\n",
      "|    critic_loss     | 4.6      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35061    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 281      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 39556    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.564    |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 38657    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 281      |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 43152    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.34     |\n",
      "|    critic_loss     | 3.04     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 42253    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 281      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 46748    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.315    |\n",
      "|    critic_loss     | 2.62     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45849    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 280      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 50344    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.263    |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49445    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 3173      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 0         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 2.2261627 |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2853          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039865985 |\n",
      "|    clip_fraction        | 0.00122       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.00108      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 139           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0002       |\n",
      "|    reward               | -0.09659374   |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 228           |\n",
      "-------------------------------------------\n",
      "day: 898, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 968785.33\n",
      "total_reward: -31214.67\n",
      "total_cost: 523405.95\n",
      "total_trades: 712\n",
      "Sharpe: 0.185\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2769         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039880797 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.000482     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 355          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    reward               | -0.0660102   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 454          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2729        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004086538 |\n",
      "|    clip_fraction        | 0.026       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 3.33e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    reward               | -0.08927948 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 313         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2707         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040666787 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00108      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 402          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000987    |\n",
      "|    reward               | -0.31424996  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 649          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2692         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009660404 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000445    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    reward               | -3.242838    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 326          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2681         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037813573 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00144     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 292          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    reward               | 0.448248     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 507          |\n",
      "------------------------------------------\n",
      "day: 898, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2040908.30\n",
      "total_reward: 1040908.30\n",
      "total_cost: 552709.67\n",
      "total_trades: 778\n",
      "Sharpe: 0.662\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2673         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026285117 |\n",
      "|    clip_fraction        | 0.0129       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000299     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    reward               | -0.054056764 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 263          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2668         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018768242 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00262      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59.7         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.000115     |\n",
      "|    reward               | 0.49634564   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2611         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052306335 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00164      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 305          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    reward               | -1.1158906   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 536          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2611        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003525573 |\n",
      "|    clip_fraction        | 0.0177      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.00242     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 73.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "day: 898, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 857651.64\n",
      "total_reward: -142348.36\n",
      "total_cost: 417830.09\n",
      "total_trades: 776\n",
      "Sharpe: 0.156\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2611       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00308996 |\n",
      "|    clip_fraction        | 0.00229    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.000287   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 427        |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.000508  |\n",
      "|    reward               | 3.2024817  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 959        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2612         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033444595 |\n",
      "|    clip_fraction        | 0.00635      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.00296      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 234          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 508          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2612        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002605031 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -0.00104    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 306         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.000444   |\n",
      "|    reward               | -0.13097948 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 571         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2605         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045076218 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.00619      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 247          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.000246     |\n",
      "|    reward               | 1.242889     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 404          |\n",
      "------------------------------------------\n",
      "day: 898, episode: 150\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1599561.82\n",
      "total_reward: 599561.82\n",
      "total_cost: 345273.01\n",
      "total_trades: 835\n",
      "Sharpe: 0.534\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2605         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041125817 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00142     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 427          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    reward               | -8.355125    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 971          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2606        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002767478 |\n",
      "|    clip_fraction        | 0.00503     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.00566     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 413         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -1.83e-05   |\n",
      "|    reward               | 7.314426    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 832         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 2607      |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 14        |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0034152 |\n",
      "|    clip_fraction        | 0.00562   |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.44     |\n",
      "|    explained_variance   | 0.00246   |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 917       |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.00113  |\n",
      "|    reward               | 0.0       |\n",
      "|    std                  | 1.02      |\n",
      "|    value_loss           | 1.96e+03  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2608        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003889985 |\n",
      "|    clip_fraction        | 0.0209      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.00183     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 874         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00232    |\n",
      "|    reward               | 25.375074   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.41e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2608         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024705003 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.00189      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 447          |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000572    |\n",
      "|    reward               | 0.475678     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 882          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 898, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2731651.60\n",
      "total_reward: 1731651.60\n",
      "total_cost: 287916.52\n",
      "total_trades: 838\n",
      "Sharpe: 0.783\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2608        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004268953 |\n",
      "|    clip_fraction        | 0.0455      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.000203    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 738         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    reward               | -0.26156336 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.38e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2609        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002682047 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.00155     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 468         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    reward               | 0.42350203  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 1.01e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2610         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013077352 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.00282      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 686          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000345    |\n",
      "|    reward               | 6.829602     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.5e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2610         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044640223 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.00244      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 912          |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    reward               | -9.201955    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 1.54e+03     |\n",
      "------------------------------------------\n",
      "day: 898, episode: 170\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3857747.10\n",
      "total_reward: 2857747.10\n",
      "total_cost: 443622.30\n",
      "total_trades: 848\n",
      "Sharpe: 0.940\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2610         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037803613 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.00105      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27e+03     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000973    |\n",
      "|    reward               | 4.921156     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.29e+03     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=50000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 387      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 3596     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.22e+03 |\n",
      "|    critic_loss     | 1.23e+05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2697     |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 333      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 7192     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.36e+03 |\n",
      "|    critic_loss     | 1.15e+05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6293     |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "day: 898, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 320      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 10788    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.05e+03 |\n",
      "|    critic_loss     | 1.15e+05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9889     |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 313      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 14384    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.72e+03 |\n",
      "|    critic_loss     | 1.15e+05 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13485    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "day: 898, episode: 190\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 309      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 17980    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.53e+03 |\n",
      "|    critic_loss     | 9.41e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17081    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 307      |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 21576    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.32e+03 |\n",
      "|    critic_loss     | 6.11e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20677    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 305      |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 25172    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.15e+03 |\n",
      "|    critic_loss     | 5.83e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24273    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "day: 898, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 303      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 28768    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.05e+03 |\n",
      "|    critic_loss     | 5.58e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27869    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 302      |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 32364    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3e+03    |\n",
      "|    critic_loss     | 5.13e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31465    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "day: 898, episode: 210\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 301      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 35960    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.97e+03 |\n",
      "|    critic_loss     | 4.25e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35061    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 301      |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 39556    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91e+03 |\n",
      "|    critic_loss     | 3.74e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 38657    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 298      |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 43152    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.85e+03 |\n",
      "|    critic_loss     | 3.11e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 42253    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "day: 898, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4134581.67\n",
      "total_reward: 3134581.67\n",
      "total_cost: 997.71\n",
      "total_trades: 898\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 295      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 46748    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.77e+03 |\n",
      "|    critic_loss     | 2.65e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45849    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 293      |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 50344    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.68e+03 |\n",
      "|    critic_loss     | 2.23e+04 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49445    |\n",
      "|    reward          | 1.288176 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 898, episode: 230\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 3596     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.89e+03 |\n",
      "|    critic_loss     | 5.75e+04 |\n",
      "|    ent_coef        | 0.131    |\n",
      "|    ent_coef_loss   | 19       |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 3495     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 7192     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 6.14e+03 |\n",
      "|    critic_loss     | 2.08e+05 |\n",
      "|    ent_coef        | 0.187    |\n",
      "|    ent_coef_loss   | 15.7     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 7091     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 10788    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.64e+03 |\n",
      "|    critic_loss     | 1.13e+05 |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | 12.3     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 10687    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 14384    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.41e+03 |\n",
      "|    critic_loss     | 6.93e+04 |\n",
      "|    ent_coef        | 0.384    |\n",
      "|    ent_coef_loss   | 9.09     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 14283    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 17980    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.76e+03 |\n",
      "|    critic_loss     | 1.33e+05 |\n",
      "|    ent_coef        | 0.551    |\n",
      "|    ent_coef_loss   | 5.65     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 17879    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 250\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 21576    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.47e+03 |\n",
      "|    critic_loss     | 6.09e+03 |\n",
      "|    ent_coef        | 0.789    |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 21475    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 25172    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.28e+03 |\n",
      "|    critic_loss     | 1.01e+05 |\n",
      "|    ent_coef        | 1.13     |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 25071    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 28768    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91e+03 |\n",
      "|    critic_loss     | 4.91e+03 |\n",
      "|    ent_coef        | 1.62     |\n",
      "|    ent_coef_loss   | -4.53    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 28667    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 32364    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.9e+03  |\n",
      "|    critic_loss     | 6.25e+03 |\n",
      "|    ent_coef        | 2.32     |\n",
      "|    ent_coef_loss   | -7.96    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 32263    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 35960    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.03e+03 |\n",
      "|    critic_loss     | 1.04e+03 |\n",
      "|    ent_coef        | 3.32     |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 35859    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 270\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 39556    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.31e+03 |\n",
      "|    critic_loss     | 4.16e+03 |\n",
      "|    ent_coef        | 4.76     |\n",
      "|    ent_coef_loss   | -14.8    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 39455    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 43152    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.77e+03 |\n",
      "|    critic_loss     | 3.95e+04 |\n",
      "|    ent_coef        | 6.82     |\n",
      "|    ent_coef_loss   | -18.2    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 43051    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 898, episode: 280\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 46748    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.58e+03 |\n",
      "|    critic_loss     | 2.07e+03 |\n",
      "|    ent_coef        | 9.78     |\n",
      "|    ent_coef_loss   | -21.3    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 46647    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=50000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample Performance\n",
    "Assume that the initial capital is $1,000,000.\n",
    "\n",
    "#### Set turbulence threshold\n",
    "Set the turbulence threshold to be greater than the maximum of insample turbulence data. If current turbulence index is greater than the threshold, then we assume that the current market is volatile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_risk_indicator = processed_full[(processed_full.date<TRAIN_END_DATE) & (processed_full.date>=TRAIN_START_DATE)]\n",
    "insample_risk_indicator = data_risk_indicator.drop_duplicates(subset=['date'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "insample_risk_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading (Out-of-sample Performance)\n",
    "We update periodically in order to take full advantage of the data, e.g., retrain quarterly, monthly or weekly. We also tune the parameters along the way, in this notebook we use the in-sample data from 2009-01 to 2020-07 to tune the parameters once, so there is some alpha decay here as the length of trade date extends.\n",
    "\n",
    "Numerous hyperparameters – e.g. the learning rate, the total number of samples to train on – influence the learning process and are usually determined by testing some variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
    "\n",
    "trained_a2c = A2C.load(\"trained_models/agent_a2c\") if if_using_a2c else None\n",
    "trained_ddpg = DDPG.load(\"trained_models/agent_ddpg\") if if_using_ddpg else None\n",
    "trained_ppo = PPO.load(\"trained_models/agent_ppo\") if if_using_ppo else None\n",
    "trained_td3 = TD3.load(\"trained_models/agent_td3\") if if_using_td3 else None\n",
    "trained_sac = SAC.load(\"trained_models/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "trained_model = trained_a2c\n",
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "trained_model = trained_ddpg\n",
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "trained_model = trained_ppo\n",
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "trained_model = trained_td3\n",
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "trained_model = trained_sac\n",
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Backtesting Results\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     a2c       ddpg           td3           ppo        sac\n",
      "date                                                                      \n",
      "2023-01-03  1.000000e+06  1000000.0  1.000000e+06  1.000000e+06  1000000.0\n",
      "2023-01-04  1.009352e+06  1000000.0  1.009352e+06  1.004914e+06  1000000.0\n",
      "2023-01-05  1.008260e+06  1000000.0  1.008260e+06  1.003350e+06  1000000.0\n",
      "2023-01-06  1.015269e+06  1000000.0  1.015269e+06  1.010359e+06  1000000.0\n",
      "2023-01-09  1.028698e+06  1000000.0  1.028698e+06  1.023788e+06  1000000.0\n"
     ]
    }
   ],
   "source": [
    "df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0])\n",
    "df_result_a2c.rename(columns = {'account_value':'a2c'}, inplace = True)\n",
    "df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0])\n",
    "df_result_ddpg.rename(columns = {'account_value':'ddpg'}, inplace = True)\n",
    "df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0])\n",
    "df_result_td3.rename(columns = {'account_value':'td3'}, inplace = True)\n",
    "df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0])\n",
    "df_result_ppo.rename(columns = {'account_value':'ppo'}, inplace = True)\n",
    "df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0])\n",
    "df_result_sac.rename(columns = {'account_value':'sac'}, inplace = True)\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result = pd.merge(result, df_result_a2c, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_ddpg, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_td3, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_ppo, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_sac, how='outer', left_index=True, right_index=True)\n",
    "print(result.head())\n",
    "# result.columns = ['a2c', 'ddpg', 'td3', 'ppo', 'sac', 'mean var', 'dji']\n",
    "\n",
    "# print(\"result: \", result)\n",
    "result.to_csv(RESULTS_DIR + \"/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
