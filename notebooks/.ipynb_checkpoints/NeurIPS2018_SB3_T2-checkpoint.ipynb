{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS2018_SB3\n",
    "\n",
    "This is a modification version based on the first notebook from [FinRL-tutorial](https://github.com/AI4Finance-Foundation/FinRL-Tutorials)\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/Stock_NeurIPS2018_SB3.ipynb\n",
    "\n",
    "## Part 1. Task Discription\n",
    "DRL agent training for cryptocurrency trading. \n",
    "\n",
    "This task is modeled as a Markov Decision Process (MDP), and the objective function is maximizing (expected) cumulative return.\n",
    "\n",
    "We specify the state-action-reward as follows:\n",
    "\n",
    "* **State s**: The states of the OHLCVT trading information consists of two co-moving assets (BTCUSD and BTCEUR).\n",
    "\n",
    "* **Action a**: The action space includes buying and selling. However, a certain limitation is included to operate on both assets. The pair shall never be actioned alone, it must come with a reversed direction.\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s', i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "**Market environment**: Cryptocurrencies from Binance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HSY/miniconda3/envs/uoa-mdt/lib/python3.10/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime, sqlite3, zipfile, os\n",
    "\n",
    "%matplotlib inline\n",
    "# from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from T2.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "check_and_make_directories([TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "\n",
    "Read data from `binance-public-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2017-08-17'\n",
    "TRAIN_END_DATE = '2022-12-31'\n",
    "TRADE_START_DATE = '2023-01-01'\n",
    "TRADE_END_DATE = '2023-07-31'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is read from SQLite database\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('../sqlite.db')\n",
    "\n",
    "table_name = 'kline_copy'\n",
    "\n",
    "# Read data from the table into a DataFrame\n",
    "query = f'SELECT * FROM {table_name};'\n",
    "sql_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Create a new DataFrame with renamed columns\n",
    "df = sql_df[['start_time', 'open', 'high', 'low', 'close', 'base_vol', 'symbol', 'id']].rename(\n",
    "    columns={\n",
    "    'start_time': 'start_time',\n",
    "    'open': 'open',\n",
    "    'high': 'high',\n",
    "    'low': 'low',\n",
    "    'close': 'close',\n",
    "    'base_vol': 'volume',\n",
    "    'symbol': 'tic',\n",
    "    'id': 'id'\n",
    "    })\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>795.150377</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>301.13</td>\n",
       "      <td>312.18</td>\n",
       "      <td>298.00</td>\n",
       "      <td>302.00</td>\n",
       "      <td>7030.710340</td>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>1199.888264</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>302.00</td>\n",
       "      <td>311.79</td>\n",
       "      <td>283.94</td>\n",
       "      <td>293.96</td>\n",
       "      <td>9537.846460</td>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>381.309763</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open     high      low    close       volume      tic  day\n",
       "0  2017-08-17  4261.48  4485.39  4200.74  4285.08   795.150377  BTCUSDT    0\n",
       "1  2017-08-17   301.13   312.18   298.00   302.00  7030.710340  ETHUSDT    0\n",
       "2  2017-08-18  4285.08  4371.52  3938.77  4108.37  1199.888264  BTCUSDT    1\n",
       "3  2017-08-18   302.00   311.79   283.94   293.96  9537.846460  ETHUSDT    1\n",
       "4  2017-08-19  4108.37  4184.69  3850.00  4139.98   381.309763  BTCUSDT    2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of symbols to merge\n",
    "symbols = ['BTCUSDT', 'ETHUSDT']\n",
    "\n",
    "# List to store individual DataFrames\n",
    "rawdfs = []\n",
    "\n",
    "# Loop through each symbol\n",
    "for symbol in symbols:\n",
    "    directory = f'../mdt_utils/binance-public-data/python/data/spot/monthly/klines/{symbol}/1d/'\n",
    "    \n",
    "    # Loop through each zip file in the directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.zip'):\n",
    "            with zipfile.ZipFile(os.path.join(directory, file_name), 'r') as zip_ref:\n",
    "                # only one CSV file in each zip archive\n",
    "                csv_file = zip_ref.namelist()[0]\n",
    "                with zip_ref.open(csv_file) as csv_fp:\n",
    "                    # Read the CSV data into a DataFrame\n",
    "                    temp_df = pd.read_csv(csv_fp, header=None)\n",
    "                    temp_df.columns = ['open_time', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "                    temp_df['date'] = pd.to_datetime(temp_df['close_time'], unit='ms').dt.strftime('%Y-%m-%d')\n",
    "                    temp_df['day'] = (pd.to_datetime(temp_df['date']) - pd.to_datetime(temp_df['date'].iloc[0])).dt.days\n",
    "                    temp_df['tic'] = symbol\n",
    "                    rawdfs.append(temp_df[['date', 'open', 'high', 'low', 'close', 'volume', 'tic', 'day']])\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "rawdf = pd.concat(rawdfs, ignore_index=True)\n",
    "\n",
    "# Count the number of unique 'tic' values per date\n",
    "tic_counts = rawdf.groupby('date')['tic'].nunique()\n",
    "\n",
    "# Filter the DataFrame to keep only rows where all 'tic' values participate\n",
    "df = rawdf[rawdf['date'].isin(tic_counts[tic_counts == len(rawdf['tic'].unique())].index)]\n",
    "\n",
    "df.sort_values(['date','tic'],ignore_index=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Preprocess Data\n",
    "\n",
    "TODO: The default feature engineering is based on date. I need to rewrite into timestamp based method\n",
    "\n",
    "The dafult [data split](https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/preprocessor/preprocessors.py) is not applicable here. Need to manually redo it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (1496, 8)\n",
      "Successfully added vix\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=False,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>795.150377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4446.630679</td>\n",
       "      <td>3946.819321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4285.080</td>\n",
       "      <td>4285.080</td>\n",
       "      <td>15.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>301.13</td>\n",
       "      <td>312.18</td>\n",
       "      <td>298.00</td>\n",
       "      <td>302.00</td>\n",
       "      <td>7030.710340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4446.630679</td>\n",
       "      <td>3946.819321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>302.000</td>\n",
       "      <td>302.000</td>\n",
       "      <td>15.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>1199.888264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.964647</td>\n",
       "      <td>4446.630679</td>\n",
       "      <td>3946.819321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4196.725</td>\n",
       "      <td>4196.725</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>302.00</td>\n",
       "      <td>311.79</td>\n",
       "      <td>283.94</td>\n",
       "      <td>293.96</td>\n",
       "      <td>9537.846460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.180385</td>\n",
       "      <td>309.350277</td>\n",
       "      <td>286.609723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>297.980</td>\n",
       "      <td>297.980</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>4069.13</td>\n",
       "      <td>4119.62</td>\n",
       "      <td>3911.79</td>\n",
       "      <td>4016.00</td>\n",
       "      <td>691.743060</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-9.655882</td>\n",
       "      <td>4325.847407</td>\n",
       "      <td>3928.440593</td>\n",
       "      <td>9.487016</td>\n",
       "      <td>-92.693236</td>\n",
       "      <td>88.718699</td>\n",
       "      <td>4127.144</td>\n",
       "      <td>4127.144</td>\n",
       "      <td>13.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      tic     open     high      low    close       volume  day  \\\n",
       "0  2017-08-17  BTCUSDT  4261.48  4485.39  4200.74  4285.08   795.150377  0.0   \n",
       "1  2017-08-17  ETHUSDT   301.13   312.18   298.00   302.00  7030.710340  0.0   \n",
       "2  2017-08-18  BTCUSDT  4285.08  4371.52  3938.77  4108.37  1199.888264  1.0   \n",
       "3  2017-08-18  ETHUSDT   302.00   311.79   283.94   293.96  9537.846460  1.0   \n",
       "4  2017-08-21  BTCUSDT  4069.13  4119.62  3911.79  4016.00   691.743060  4.0   \n",
       "\n",
       "       macd      boll_ub      boll_lb    rsi_30     cci_30       dx_30  \\\n",
       "0  0.000000  4446.630679  3946.819321  0.000000 -66.666667  100.000000   \n",
       "1  0.000000  4446.630679  3946.819321  0.000000 -66.666667  100.000000   \n",
       "2 -3.964647  4446.630679  3946.819321  0.000000 -66.666667  100.000000   \n",
       "3 -0.180385   309.350277   286.609723  0.000000 -66.666667  100.000000   \n",
       "4 -9.655882  4325.847407  3928.440593  9.487016 -92.693236   88.718699   \n",
       "\n",
       "   close_30_sma  close_60_sma    vix  \n",
       "0      4285.080      4285.080  15.55  \n",
       "1       302.000       302.000  15.55  \n",
       "2      4196.725      4196.725  14.26  \n",
       "3       297.980       297.980  14.26  \n",
       "4      4127.144      4127.144  13.19  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full.sort_values(['date','tic'],ignore_index=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Build A Market Environment in OpenAI Gym-style\n",
    "The training process involves observing cryptocurrency price change, taking an action and reward's calculation. By interacting with the market environment, the agent will eventually derive a trading strategy that may maximize (expected) rewards.\n",
    "\n",
    "Our market environment, based on OpenAI Gym, simulates stock markets with historical market data.\n",
    "\n",
    "### Data Split\n",
    "We split the data into training set and testing set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data length: 2706\n",
      "Trade Data Length: 286\n",
      "Indicators: ['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma']\n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE, TRADE_END_DATE)\n",
    "train_length = len(train)\n",
    "trade_length = len(trade)\n",
    "print(f\"Training Data length: {train_length}\")\n",
    "print(f\"Trade Data Length: {trade_length}\")\n",
    "print(f\"Indicators: {INDICATORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are trading with 2 assets as a pair, therefore we have only 1 pair_dimension\n",
    "\n",
    "> `1`: Represents the cash balance. There's one element in the state for the agent's cash.\n",
    ">\n",
    "> `2 * stock_dimension`: Represents the stock prices and stock ownership. There are two elements for each stock: one for the stock price and one for the number of shares owned.\n",
    ">\n",
    "> `len(INDICATORS) * stock_dimension`: Represents the technical indicators for each stock. Each indicator contributes one element per stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock Dimension: 2, State Space: 21\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "if stock_dimension != 2:\n",
    "    raise ValueError(\"Stock dimension must be equal to 2 for pair trading.\")\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 1, # int, maximum number of coins to trade\n",
    "    \"initial_amount\": 100000, # start with 1000000 USDT\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list, # transaction cost percentage per trade\n",
    "    \"sell_cost_pct\": sell_cost_list, # transaction cost percentage per trade\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": 2, # we will always have 2 stocks\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": 2, # we only allow the trade to give a single action\n",
    "    \"reward_scaling\": 1e-4 # scaling factor for reward, good for training\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of state\n",
    "```\n",
    "94502.19235000112: This is the available cash balance that the agent has for making trading decisions.\n",
    "24948.21: The price of the first stock in your trading pair (e.g., BTC-GBP).\n",
    "21668.05: The price of the second stock in your trading pair (e.g., BTC-EUR).\n",
    "103: The number of shares the agent holds for the first stock (e.g., BTC-GBP).\n",
    "0: The number of shares the agent holds for the second stock (e.g., BTC-EUR).\n",
    "-299.9631790789099: A technical indicator value.\n",
    "-345.112034646816: A technical indicator value.\n",
    "26817.756609157783: A technical indicator value.\n",
    "23644.528099789444: A technical indicator value.\n",
    "24186.333390842214: A technical indicator value.\n",
    "20930.97290021056: A technical indicator value.\n",
    "48.63796032144048: A technical indicator value.\n",
    "47.40677058214158: A technical indicator value.\n",
    "-74.29641721884254: A technical indicator value.\n",
    "-89.66529346827706: A technical indicator value.\n",
    "12.572876815282788: A technical indicator value.\n",
    "16.60796108870377: A technical indicator value.\n",
    "25597.621333333333: A technical indicator value.\n",
    "22452.637666666666: A technical indicator value.\n",
    "25993.954166666666: A technical indicator value.\n",
    "22842.914500000003: A technical indicator value.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100000,\n",
       " 4285.08,\n",
       " 302.0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4446.630678606951,\n",
       " 4446.630678606951,\n",
       " 3946.81932139305,\n",
       " 3946.81932139305,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -66.666666666667,\n",
       " -66.666666666667,\n",
       " 100.0,\n",
       " 100.0,\n",
       " 4285.08,\n",
       " 302.0,\n",
       " 4285.08,\n",
       " 302.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train_gym.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Train DRL Agents\n",
    "* The DRL algorithms are from Stable Baselines 3. Users are also encouraged to try ElegantRL and Ray RLlib.\n",
    "* FinRL includes fine-tuned standard DRL algorithms, such as DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "design their own DRL algorithms by adapting these DRL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 1: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 722      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.93    |\n",
      "|    explained_variance | -23      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.584    |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 760         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.94       |\n",
      "|    explained_variance | -0.231      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 0.812       |\n",
      "|    reward             | -0.14691672 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.202       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.95      |\n",
      "|    explained_variance | 0.067      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -0.781     |\n",
      "|    reward             | 0.22217578 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.113      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 720        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.96      |\n",
      "|    explained_variance | -0.161     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.74      |\n",
      "|    reward             | -0.7531018 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 740       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.96     |\n",
      "|    explained_variance | 0.0662    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 16.7      |\n",
      "|    reward             | 4.8155503 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 49.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 753         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.97       |\n",
      "|    explained_variance | -3.35       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.053      |\n",
      "|    reward             | -0.02276705 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.0196      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 762          |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 4            |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3           |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -0.0867      |\n",
      "|    reward             | -0.028306445 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 0.0013       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 771         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3          |\n",
      "|    explained_variance | -0.408      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 0.296       |\n",
      "|    reward             | -0.42442262 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.0496      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 778          |\n",
      "|    iterations         | 900          |\n",
      "|    time_elapsed       | 5            |\n",
      "|    total_timesteps    | 4500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.01        |\n",
      "|    explained_variance | -0.154       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 899          |\n",
      "|    policy_loss        | 4.34         |\n",
      "|    reward             | -0.046069205 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 2.5          |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 769        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.02      |\n",
      "|    explained_variance | 0.0732     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 0.852      |\n",
      "|    reward             | -0.9540041 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.837      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 758         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.04       |\n",
      "|    explained_variance | -0.618      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -0.247      |\n",
      "|    reward             | 0.011068515 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.0257      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 765       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.05     |\n",
      "|    explained_variance | 0.555     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.27     |\n",
      "|    reward             | 0.4142707 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.0113    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 771         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.05       |\n",
      "|    explained_variance | -0.817      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -10.1       |\n",
      "|    reward             | -0.22012651 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 10.7        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 776          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 9            |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.06        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | 0.17         |\n",
      "|    reward             | -0.023581315 |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 0.00895      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 781        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.09      |\n",
      "|    explained_variance | -0.0343    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -0.0815    |\n",
      "|    reward             | 0.06194723 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.00102    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 777        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.11      |\n",
      "|    explained_variance | 0.218      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 0.363      |\n",
      "|    reward             | 0.09206127 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.0316     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 737          |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 11           |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.13        |\n",
      "|    explained_variance | -22.3        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | 0.0651       |\n",
      "|    reward             | -0.008848154 |\n",
      "|    std                | 1.16         |\n",
      "|    value_loss         | 0.00401      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.15      |\n",
      "|    explained_variance | -0.1       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 1.37       |\n",
      "|    reward             | 0.13681723 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 0.645      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 697      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.15    |\n",
      "|    explained_variance | 0.492    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0135  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 1.59e-05 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 698          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 14           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.16        |\n",
      "|    explained_variance | -0.00955     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | -0.847       |\n",
      "|    reward             | -0.010346174 |\n",
      "|    std                | 1.18         |\n",
      "|    value_loss         | 0.0624       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.16      |\n",
      "|    explained_variance | -0.212     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -1.56      |\n",
      "|    reward             | -1.7961789 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.284      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 705         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.18       |\n",
      "|    explained_variance | -0.0194     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 5.61        |\n",
      "|    reward             | -0.26503754 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 4.55        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 709        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.2       |\n",
      "|    explained_variance | 0.275      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -0.847     |\n",
      "|    reward             | 0.43640494 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 0.2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 714        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.19      |\n",
      "|    explained_variance | -0.0343    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | -13.1      |\n",
      "|    reward             | -0.1581501 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 34.2       |\n",
      "--------------------------------------\n",
      "day: 1352, episode: 10\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 417694.77\n",
      "total_reward: 317694.77\n",
      "total_cost: 3522.30\n",
      "total_trades: 5408\n",
      "Sharpe: 0.782\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 698        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.18      |\n",
      "|    explained_variance | 0.0409     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -2.78      |\n",
      "|    reward             | 0.29979783 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.655      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 697        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.18      |\n",
      "|    explained_variance | -0.0129    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 2          |\n",
      "|    reward             | -0.2428251 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 0.573      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.17      |\n",
      "|    explained_variance | 0.304      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -6.41      |\n",
      "|    reward             | 0.03438371 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 4.96       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 703        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.19      |\n",
      "|    explained_variance | 0.24       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 8.35       |\n",
      "|    reward             | 0.33246893 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 6.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 704       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.2      |\n",
      "|    explained_variance | -0.0178   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -13.5     |\n",
      "|    reward             | -4.143793 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 19.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 703         |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.22       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | -0.022      |\n",
      "|    reward             | -0.09598101 |\n",
      "|    std                | 1.21        |\n",
      "|    value_loss         | 0.000401    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 699         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.24       |\n",
      "|    explained_variance | 0.184       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | -0.629      |\n",
      "|    reward             | -0.17064168 |\n",
      "|    std                | 1.23        |\n",
      "|    value_loss         | 0.09        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.24      |\n",
      "|    explained_variance | -0.168     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -3.78      |\n",
      "|    reward             | 0.34150034 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 0.946      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 703        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.25      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 0.157      |\n",
      "|    reward             | -0.0589398 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 0.00222    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 706        |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.26      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -0.56      |\n",
      "|    reward             | 0.13396803 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 0.117      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 709        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.27      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | -2.41      |\n",
      "|    reward             | 0.20629515 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 0.904      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 712      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0546   |\n",
      "|    reward             | 0.023195 |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 0.000218 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 715       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.34     |\n",
      "|    explained_variance | 0.185     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -1.44     |\n",
      "|    reward             | -0.002011 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 0.228     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 717      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -0.0212  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 4.89e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 720       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.38     |\n",
      "|    explained_variance | -1.59e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 0.0342    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 0.000255  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 721      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.43    |\n",
      "|    explained_variance | -172     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.0335   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 0.000421 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 1.18e-05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 721      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.00164 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 2.62e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 722      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.00578 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 3.37e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 721      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.76    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -0.00458 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.58     |\n",
      "|    value_loss         | 1.71e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 721      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.87    |\n",
      "|    explained_variance | -29.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.00688 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 0.000124 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 721       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -0.000752 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.76      |\n",
      "|    value_loss         | 5.69e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 722       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -0.000526 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.86      |\n",
      "|    value_loss         | 2.4e-08   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 723      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.98     |\n",
      "|    value_loss         | 3.67e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 722       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.3      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -0.000641 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.08      |\n",
      "|    value_loss         | 1.84e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.000107 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.2      |\n",
      "|    value_loss         | 5.7e-10  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 726      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -0.00853 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.32     |\n",
      "|    value_loss         | 2.34e-06 |\n",
      "------------------------------------\n",
      "day: 1352, episode: 20\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 98902.89\n",
      "total_reward: -1097.11\n",
      "total_cost: 17.88\n",
      "total_trades: 5408\n",
      "Sharpe: -0.088\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 720           |\n",
      "|    iterations         | 5200          |\n",
      "|    time_elapsed       | 36            |\n",
      "|    total_timesteps    | 26000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -4.58         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5199          |\n",
      "|    policy_loss        | 0.00331       |\n",
      "|    reward             | -9.187235e-05 |\n",
      "|    std                | 2.39          |\n",
      "|    value_loss         | 5.16e-07      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -4.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -0.0595    |\n",
      "|    reward             | 0.11117334 |\n",
      "|    std                | 2.44       |\n",
      "|    value_loss         | 0.000981   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 723          |\n",
      "|    iterations         | 5400         |\n",
      "|    time_elapsed       | 37           |\n",
      "|    total_timesteps    | 27000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -4.61        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5399         |\n",
      "|    policy_loss        | 1.03         |\n",
      "|    reward             | -0.072612815 |\n",
      "|    std                | 2.42         |\n",
      "|    value_loss         | 0.0556       |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | -0.00214 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.46     |\n",
      "|    value_loss         | 1.79e-07 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 726           |\n",
      "|    iterations         | 5600          |\n",
      "|    time_elapsed       | 38            |\n",
      "|    total_timesteps    | 28000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -4.68         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5599          |\n",
      "|    policy_loss        | -0.0257       |\n",
      "|    reward             | -0.0010513263 |\n",
      "|    std                | 2.52          |\n",
      "|    value_loss         | 0.00242       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 712         |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 40          |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -4.72       |\n",
      "|    explained_variance | -0.227      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | 0.355       |\n",
      "|    reward             | 0.004559833 |\n",
      "|    std                | 2.56        |\n",
      "|    value_loss         | 0.00789     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 707          |\n",
      "|    iterations         | 5800         |\n",
      "|    time_elapsed       | 40           |\n",
      "|    total_timesteps    | 29000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -4.77        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5799         |\n",
      "|    policy_loss        | 0.0136       |\n",
      "|    reward             | -0.021463297 |\n",
      "|    std                | 2.63         |\n",
      "|    value_loss         | 2.83e-05     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 708        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -4.79      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | -5.88      |\n",
      "|    reward             | -0.8704792 |\n",
      "|    std                | 2.65       |\n",
      "|    value_loss         | 1.92       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 710      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.8     |\n",
      "|    explained_variance | -19.1    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -0.125   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.67     |\n",
      "|    value_loss         | 0.000645 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 711      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 42       |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.84    |\n",
      "|    explained_variance | 1.25e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 0.00278  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.72     |\n",
      "|    value_loss         | 3.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 713      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.89    |\n",
      "|    explained_variance | -246     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | -0.026   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.8      |\n",
      "|    value_loss         | 5.68e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 711      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.97    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.000365 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.91     |\n",
      "|    value_loss         | 7.08e-09 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 713         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.02       |\n",
      "|    explained_variance | 0.0846      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | 4.69        |\n",
      "|    reward             | -0.19379506 |\n",
      "|    std                | 2.98        |\n",
      "|    value_loss         | 3.29        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 714      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.01    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.00702  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.97     |\n",
      "|    value_loss         | 2.1e-06  |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 716         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.05       |\n",
      "|    explained_variance | 0.208       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -0.182      |\n",
      "|    reward             | -0.24060829 |\n",
      "|    std                | 3.03        |\n",
      "|    value_loss         | 0.00168     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 718        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.08      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -8.31      |\n",
      "|    reward             | -0.2520449 |\n",
      "|    std                | 3.07       |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 719      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | -0.0289  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.12     |\n",
      "|    value_loss         | 4.01e-05 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 720         |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 47          |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.14       |\n",
      "|    explained_variance | 0.182       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | 0.213       |\n",
      "|    reward             | 0.013451921 |\n",
      "|    std                | 3.17        |\n",
      "|    value_loss         | 0.0177      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.17      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | -19.8      |\n",
      "|    reward             | 0.62417954 |\n",
      "|    std                | 3.21       |\n",
      "|    value_loss         | 19.4       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 723      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.19    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.24     |\n",
      "|    value_loss         | 1.71e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.23     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -0.000166 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.3       |\n",
      "|    value_loss         | 1.22e-09  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 725         |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.25       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | -10.4       |\n",
      "|    reward             | -0.19362137 |\n",
      "|    std                | 3.35        |\n",
      "|    value_loss         | 4.84        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 726         |\n",
      "|    iterations         | 7400        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 37000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.27       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7399        |\n",
      "|    policy_loss        | 2.82        |\n",
      "|    reward             | -0.62525964 |\n",
      "|    std                | 3.37        |\n",
      "|    value_loss         | 0.321       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 728        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.29      |\n",
      "|    explained_variance | 0.0515     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -1.02      |\n",
      "|    reward             | 0.42913285 |\n",
      "|    std                | 3.41       |\n",
      "|    value_loss         | 0.46       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 729         |\n",
      "|    iterations         | 7600        |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 38000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.28       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7599        |\n",
      "|    policy_loss        | -0.215      |\n",
      "|    reward             | -0.09167012 |\n",
      "|    std                | 3.39        |\n",
      "|    value_loss         | 0.00817     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 730        |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.3       |\n",
      "|    explained_variance | -2.92      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | -4.05      |\n",
      "|    reward             | 0.31949008 |\n",
      "|    std                | 3.42       |\n",
      "|    value_loss         | 0.544      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 731       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.32     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 3.74      |\n",
      "|    reward             | 0.2737993 |\n",
      "|    std                | 3.46      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "day: 1352, episode: 30\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 371165.23\n",
      "total_reward: 271165.23\n",
      "total_cost: 66.76\n",
      "total_trades: 5408\n",
      "Sharpe: 0.764\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.34      |\n",
      "|    explained_variance | -0.368     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 1.47       |\n",
      "|    reward             | 0.27786043 |\n",
      "|    std                | 3.49       |\n",
      "|    value_loss         | 0.101      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 54          |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.35       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | 1.25        |\n",
      "|    reward             | 0.111317724 |\n",
      "|    std                | 3.51        |\n",
      "|    value_loss         | 0.0618      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 8100        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 40500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.37       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8099        |\n",
      "|    policy_loss        | -1.96       |\n",
      "|    reward             | -0.07552685 |\n",
      "|    std                | 3.55        |\n",
      "|    value_loss         | 0.116       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.39       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | 0.221       |\n",
      "|    reward             | 0.043309502 |\n",
      "|    std                | 3.59        |\n",
      "|    value_loss         | 0.00207     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 734            |\n",
      "|    iterations         | 8300           |\n",
      "|    time_elapsed       | 56             |\n",
      "|    total_timesteps    | 41500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -5.4           |\n",
      "|    explained_variance | 0.135          |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 8299           |\n",
      "|    policy_loss        | -10.9          |\n",
      "|    reward             | -0.00087746076 |\n",
      "|    std                | 3.61           |\n",
      "|    value_loss         | 4.4            |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.44      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | 1.23       |\n",
      "|    reward             | 0.04203522 |\n",
      "|    std                | 3.66       |\n",
      "|    value_loss         | 0.0666     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 736        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.45      |\n",
      "|    explained_variance | 0.027      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | 13.9       |\n",
      "|    reward             | 0.33067122 |\n",
      "|    std                | 3.68       |\n",
      "|    value_loss         | 6.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.47     |\n",
      "|    explained_variance | -0.0187   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -0.143    |\n",
      "|    reward             | 3.7493126 |\n",
      "|    std                | 3.73      |\n",
      "|    value_loss         | 26.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 8700        |\n",
      "|    time_elapsed       | 59          |\n",
      "|    total_timesteps    | 43500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.47       |\n",
      "|    explained_variance | 0.435       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8699        |\n",
      "|    policy_loss        | -0.0282     |\n",
      "|    reward             | -0.31684458 |\n",
      "|    std                | 3.73        |\n",
      "|    value_loss         | 0.00089     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 731        |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.47      |\n",
      "|    explained_variance | -0.0522    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 1.01       |\n",
      "|    reward             | -0.1497792 |\n",
      "|    std                | 3.74       |\n",
      "|    value_loss         | 0.0846     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 727       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.46     |\n",
      "|    explained_variance | -0.000447 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -3.13     |\n",
      "|    reward             | 2.4090571 |\n",
      "|    std                | 3.72      |\n",
      "|    value_loss         | 0.555     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 727        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.48      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 0.0311     |\n",
      "|    reward             | -0.1497729 |\n",
      "|    std                | 3.74       |\n",
      "|    value_loss         | 0.00185    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 728      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -5.49    |\n",
      "|    explained_variance | -1.12    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 23.8     |\n",
      "|    reward             | 2.081238 |\n",
      "|    std                | 3.76     |\n",
      "|    value_loss         | 17.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 728       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.49     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -0.804    |\n",
      "|    reward             | 0.076102  |\n",
      "|    std                | 3.76      |\n",
      "|    value_loss         | 0.232     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 728          |\n",
      "|    iterations         | 9300         |\n",
      "|    time_elapsed       | 63           |\n",
      "|    total_timesteps    | 46500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -5.5         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9299         |\n",
      "|    policy_loss        | 0.426        |\n",
      "|    reward             | -0.041200265 |\n",
      "|    std                | 3.78         |\n",
      "|    value_loss         | 0.00797      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 729       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.5      |\n",
      "|    explained_variance | 0.0944    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | 1.0307671 |\n",
      "|    std                | 3.79      |\n",
      "|    value_loss         | 13.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 729         |\n",
      "|    iterations         | 9500        |\n",
      "|    time_elapsed       | 65          |\n",
      "|    total_timesteps    | 47500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -5.5        |\n",
      "|    explained_variance | -0.199      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9499        |\n",
      "|    policy_loss        | -6.35       |\n",
      "|    reward             | 0.015300463 |\n",
      "|    std                | 3.78        |\n",
      "|    value_loss         | 1.6         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 729        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.5       |\n",
      "|    explained_variance | 0.000905   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 1.08       |\n",
      "|    reward             | -2.9705684 |\n",
      "|    std                | 3.79       |\n",
      "|    value_loss         | 0.0476     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 728       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.49     |\n",
      "|    explained_variance | 0.0103    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 14.9      |\n",
      "|    reward             | -0.708565 |\n",
      "|    std                | 3.77      |\n",
      "|    value_loss         | 7.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 729        |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.49      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | 0.392      |\n",
      "|    reward             | 0.18840584 |\n",
      "|    std                | 3.77       |\n",
      "|    value_loss         | 0.00494    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 730        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -5.5       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -1.01      |\n",
      "|    reward             | 0.13205995 |\n",
      "|    std                | 3.78       |\n",
      "|    value_loss         | 0.0327     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 731          |\n",
      "|    iterations         | 10000        |\n",
      "|    time_elapsed       | 68           |\n",
      "|    total_timesteps    | 50000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -5.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9999         |\n",
      "|    policy_loss        | 1.34         |\n",
      "|    reward             | -0.029844737 |\n",
      "|    std                | 3.8          |\n",
      "|    value_loss         | 0.165        |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=50000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "#     \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample Performance\n",
    "Assume that the initial capital is $1,000,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading (Out-of-sample Performance)\n",
    "We update periodically in order to take full advantage of the data, e.g., retrain quarterly, monthly or weekly. We also tune the parameters along the way, in this notebook we use the in-sample data from 2009-01 to 2020-07 to tune the parameters once, so there is some alpha decay here as the length of trade date extends.\n",
    "\n",
    "Numerous hyperparameters – e.g. the learning rate, the total number of samples to train on – influence the learning process and are usually determined by testing some variations.\n",
    "\n",
    "The Env\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/env_stock_trading/env_stocktrading.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_kwargs = {\n",
    "#     \"hmax\": 1, # int, maximum number of bitcoins to trade\n",
    "#     \"initial_amount\": 1000000, # start money\n",
    "#     \"num_stock_shares\": num_stock_shares,\n",
    "#     \"buy_cost_pct\": buy_cost_list, # transaction cost percentage per trade\n",
    "#     \"sell_cost_pct\": sell_cost_list, # transaction cost percentage per trade\n",
    "#     \"state_space\": state_space,\n",
    "#     \"stock_dim\": 2, # we will always have 2 stocks\n",
    "#     \"tech_indicator_list\": INDICATORS,\n",
    "#     \"action_space\": 1, # we only allow the trade to give a single action\n",
    "#     \"reward_scaling\": 1e-4 # scaling factor for reward, good for training\n",
    "# }\n",
    "\n",
    "e_trade_gym = StockTradingEnv(df = trade, risk_indicator_col='vix', **env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
    "\n",
    "trained_a2c = A2C.load(\"trained_models/agent_a2c\") if if_using_a2c else None\n",
    "trained_ddpg = DDPG.load(\"trained_models/agent_ddpg\") if if_using_ddpg else None\n",
    "trained_ppo = PPO.load(\"trained_models/agent_ppo\") if if_using_ppo else None\n",
    "trained_td3 = TD3.load(\"trained_models/agent_td3\") if if_using_td3 else None\n",
    "trained_sac = SAC.load(\"trained_models/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_a2c\n",
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_ddpg\n",
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_ppo\n",
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_td3\n",
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trained_model = trained_sac\n",
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_model, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Backtesting Results\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_a2c = df_account_value_a2c.set_index(df_account_value_a2c.columns[0])\n",
    "df_result_a2c.rename(columns = {'account_value':'a2c'}, inplace = True)\n",
    "df_result_ddpg = df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0])\n",
    "df_result_ddpg.rename(columns = {'account_value':'ddpg'}, inplace = True)\n",
    "df_result_td3 = df_account_value_td3.set_index(df_account_value_td3.columns[0])\n",
    "df_result_td3.rename(columns = {'account_value':'td3'}, inplace = True)\n",
    "df_result_ppo = df_account_value_ppo.set_index(df_account_value_ppo.columns[0])\n",
    "df_result_ppo.rename(columns = {'account_value':'ppo'}, inplace = True)\n",
    "# df_result_sac = df_account_value_sac.set_index(df_account_value_sac.columns[0])\n",
    "# df_result_sac.rename(columns = {'account_value':'sac'}, inplace = True)\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result = pd.merge(result, df_result_a2c, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_ddpg, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_td3, how='outer', left_index=True, right_index=True)\n",
    "result = pd.merge(result, df_result_ppo, how='outer', left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_result_sac, how='outer', left_index=True, right_index=True)\n",
    "print(result.head())\n",
    "# result.columns = ['a2c', 'ddpg', 'td3', 'ppo', 'sac', 'mean var', 'dji']\n",
    "\n",
    "# print(\"result: \", result)\n",
    "result.to_csv(RESULTS_DIR + \"/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
